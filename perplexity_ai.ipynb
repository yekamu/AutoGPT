{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yekamu/AutoGPT/blob/master/perplexity_ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hUVHcPUiUbR"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87uGV8mFiUbT"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import threading\n",
        "import requests\n",
        "import traceback\n",
        "from fastapi import HTTPException\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yh__80H4iUbW"
      },
      "source": [
        "# API KEYS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0KlXsMHiUbX"
      },
      "outputs": [],
      "source": [
        "# In the context of Google search, \"cx\" stands for \"Custom Search Engine ID.\"\n",
        "# When you perform a search using Google's Custom Search Engine (CSE), you can create a custom search engine for your website or application. Google provides an ID (cx) for each custom search engine you create.\n",
        "# This ID is used to uniquely identify your custom search engine when making search requests via the Google Custom Search JSON API.\n",
        "CX = \"\"\n",
        "\n",
        "# Google Search API. Free but have rate limits\n",
        "SERPER_API = \"\"\n",
        "GOOGLE_SEARCH_API = \"\"\n",
        "\n",
        "# AI Models API call\n",
        "DEEPSEEK_API = \"\"\n",
        "LEPTON_API = \"\"\n",
        "GROQ_API = \"\"\n",
        "OpenAI_API = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Em6N_8vdiUbZ"
      },
      "source": [
        "# ENDPOINTS & PARAMS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6Sz7J0miUba"
      },
      "outputs": [],
      "source": [
        "# Search engine related.\n",
        "# Paid\n",
        "BING_SEARCH_V7_ENDPOINT = \"https://api.bing.microsoft.com/v7.0/search\"\n",
        "SEARCHAPI_SEARCH_ENDPOINT = \"https://www.searchapi.io/api/v1/search\"\n",
        "\n",
        "# Free\n",
        "SERPER_SEARCH_ENDPOINT = \"https://google.serper.dev/search\"\n",
        "GOOGLE_SEARCH_ENDPOINT = \"https://customsearch.googleapis.com/customsearch/v1\"\n",
        "\n",
        "# Specify the number of references from the search engine you want to use.\n",
        "REFERENCE_COUNT = 8\n",
        "\n",
        "# Specify the default timeout for the search engine. If the search engine does not respond within this time, we will return an error.\n",
        "DEFAULT_SEARCH_ENGINE_TIMEOUT = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRe1sNfjiUbb"
      },
      "source": [
        "# PROMPTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlrZr2nsiUbb"
      },
      "outputs": [],
      "source": [
        "# This is really the most important part of the rag model. It gives instructions\n",
        "# to the model on how to generate the answer. Of course, different models may\n",
        "# behave differently, and we haven't tuned the prompt to make it optimal - this\n",
        "# is left to you, application creators, as an open problem.\n",
        "_rag_query_text = \"\"\"\n",
        "You are a large language AI assistant built by Devs Do Code. You are given a user question, and please write clean, concise and accurate answer to the question. You will be given a set of related contexts to the question, each starting with a reference number like [[citation:x]], where x is a number. Please use the context and cite the context at the end of each sentence if applicable.\n",
        "\n",
        "Your answer must be correct, accurate and written by an expert using an unbiased and professional tone. Please limit to 1024 tokens. Do not give any information that is not related to the question, and do not repeat. Say \"information is missing on\" followed by the related topic, if the given context do not provide sufficient information.\n",
        "\n",
        "Please cite the contexts with the reference numbers, in the format [citation:x]. If a sentence comes from multiple contexts, please list all applicable citations, like [citation:3][citation:5]. Other than code and specific names and citations, your answer must be written in the same language as the question. If there are too many citations, choose the best of them\n",
        "Here are the set of contexts:\n",
        "\n",
        "{context}\n",
        "\n",
        "Remember, don't blindly repeat the contexts. And here is the user question:\n",
        "\"\"\"\n",
        "\n",
        "# A set of stop words to use - this is not a complete set, and you may want to\n",
        "# add more given your observation.\n",
        "stop_words = [\n",
        "    \"<|im_end|>\",\n",
        "    \"[End]\",\n",
        "    \"[end]\",\n",
        "    \"\\nReferences:\\n\",\n",
        "    \"\\nSources:\\n\",\n",
        "    \"End.\",\n",
        "]\n",
        "\n",
        "# This is the prompt that asks the model to generate related questions to the\n",
        "# original question and the contexts.\n",
        "# Ideally, one want to include both the original question and the answer from the\n",
        "# model, but we are not doing that here: if we need to wait for the answer, then\n",
        "# the generation of the related questions will usually have to start only after\n",
        "# the whole answer is generated. This creates a noticeable delay in the response\n",
        "# time. As a result, and as you will see in the code, we will be sending out two\n",
        "# consecutive requests to the model: one for the answer, and one for the related\n",
        "# questions. This is not ideal, but it is a good tradeoff between response time\n",
        "# and quality.\n",
        "_more_questions_prompt = \"\"\"\n",
        "You are a helpful assistant that helps the user to ask related questions, based on user's original question and the related contexts. Please identify worthwhile topics that can be follow-ups, and write questions no longer than 20 words each. Please make sure that specifics, like events, names, locations, are included in follow up questions so they can be asked standalone. For example, if the original question asks about \"the Manhattan project\", in the follow up question, do not just say \"the project\", but use the full name \"the Manhattan project\". The format of giving the responses and generating the questions shoudld be like this:\n",
        "\n",
        "1. [Question 1]\n",
        "2. [Question 2]\n",
        "3. [Question 3]\n",
        "\n",
        "Here are the contexts of the question:\n",
        "\n",
        "{context}\n",
        "\n",
        "Remember, based on the original question and related contexts, suggest three such further questions. Do NOT repeat the original question. Each related question should be no longer than 20 words. Here is the original question:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LECh6OWdiUbd"
      },
      "source": [
        "# CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQEOt8PaiUbe"
      },
      "source": [
        "#### WEB SEARCH FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51dUVJtsiUbe"
      },
      "outputs": [],
      "source": [
        "def search_with_serper(query: str, subscription_key=SERPER_API, prints=False):\n",
        "    \"\"\"\n",
        "    Search with serper and return the contexts.\n",
        "    \"\"\"\n",
        "    payload = json.dumps({\n",
        "        \"q\": query,\n",
        "        \"num\": (\n",
        "            REFERENCE_COUNT\n",
        "            if REFERENCE_COUNT % 10 == 0\n",
        "            else (REFERENCE_COUNT // 10 + 1) * 10\n",
        "        ),\n",
        "    })\n",
        "    headers = {\"X-API-KEY\": subscription_key, \"Content-Type\": \"application/json\"}\n",
        "    response = requests.post(\n",
        "        SERPER_SEARCH_ENDPOINT,\n",
        "        headers=headers,\n",
        "        data=payload,\n",
        "        timeout=DEFAULT_SEARCH_ENGINE_TIMEOUT,\n",
        "    )\n",
        "    if not response.ok:\n",
        "        raise HTTPException(response.status_code, \"Search engine error.\")\n",
        "    json_content = response.json()\n",
        "\n",
        "    if prints:\n",
        "        print(json_content)\n",
        "        print(\"\\n\\n\\n-------------------------------------------------------------------------------\\n\\n\\n\")\n",
        "\n",
        "    try:\n",
        "        # convert to the same format as bing/google\n",
        "        contexts = []\n",
        "        if json_content.get(\"knowledgeGraph\"):\n",
        "            url = json_content[\"knowledgeGraph\"].get(\"descriptionUrl\") or json_content[\"knowledgeGraph\"].get(\"website\")\n",
        "            snippet = json_content[\"knowledgeGraph\"].get(\"description\")\n",
        "            if url and snippet:\n",
        "                contexts.append({\n",
        "                    \"name\": json_content[\"knowledgeGraph\"].get(\"title\",\"\"),\n",
        "                    \"url\": url,\n",
        "                    \"snippet\": snippet\n",
        "                })\n",
        "        if json_content.get(\"answerBox\"):\n",
        "            url = json_content[\"answerBox\"].get(\"url\")\n",
        "            snippet = json_content[\"answerBox\"].get(\"snippet\") or json_content[\"answerBox\"].get(\"answer\")\n",
        "            if url and snippet:\n",
        "                contexts.append({\n",
        "                    \"name\": json_content[\"answerBox\"].get(\"title\",\"\"),\n",
        "                    \"url\": url,\n",
        "                    \"snippet\": snippet\n",
        "                })\n",
        "        contexts += [\n",
        "            {\"name\": c[\"title\"], \"url\": c[\"link\"], \"snippet\": c.get(\"snippet\",\"\")}\n",
        "            for c in json_content[\"organic\"]\n",
        "        ]\n",
        "\n",
        "        if prints:\n",
        "            print(contexts[:REFERENCE_COUNT])\n",
        "        return contexts[:REFERENCE_COUNT]\n",
        "\n",
        "    except KeyError:\n",
        "        return []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8S_p_44iUbf"
      },
      "outputs": [],
      "source": [
        "def search_with_google(query: str, subscription_key= GOOGLE_SEARCH_API, cx=CX ):\n",
        "    \"\"\"\n",
        "    Search with google and return the contexts.\n",
        "    \"\"\"\n",
        "    params = {\n",
        "        \"key\": subscription_key,\n",
        "        \"cx\": cx,\n",
        "        \"q\": query,\n",
        "        \"num\": REFERENCE_COUNT,\n",
        "    }\n",
        "    response = requests.get(\n",
        "        GOOGLE_SEARCH_ENDPOINT, params=params, timeout=DEFAULT_SEARCH_ENGINE_TIMEOUT\n",
        "    )\n",
        "    if not response.ok:\n",
        "        raise HTTPException(response.status_code, \"Search engine error.\")\n",
        "    json_content = response.json()\n",
        "    try:\n",
        "        contexts = json_content[\"items\"][:REFERENCE_COUNT]\n",
        "    except KeyError:\n",
        "        return []\n",
        "    print(contexts)\n",
        "    # return contexts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcfP-I1EiUbf"
      },
      "outputs": [],
      "source": [
        "def extract_citation_numbers(sentence):\n",
        "    # Define a regular expression pattern to match citation numbers\n",
        "    pattern = r'\\[citation:(\\d+)\\]'\n",
        "\n",
        "    # Use re.findall() to extract all citation numbers from the sentence\n",
        "    citation_numbers = re.findall(pattern, sentence)\n",
        "\n",
        "    # Return the extracted citation numbers as a list\n",
        "    return citation_numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V23DPqQxiUbg"
      },
      "outputs": [],
      "source": [
        "def fetch_json_attributes(json_data, print=False):\n",
        "\n",
        "    # Initialize empty lists for each key\n",
        "    names = []\n",
        "    urls = []\n",
        "    snippets = []\n",
        "\n",
        "    # Iterate over each item in the list and extract values for each key\n",
        "    for item in json_data:\n",
        "        names.append(item['name'])\n",
        "        urls.append(item['url'])\n",
        "        snippets.append(item['snippet'])\n",
        "\n",
        "    if print:\n",
        "        # Print the extracted values\n",
        "        print(\"Names:\", names)\n",
        "        print(\"URLs:\", urls)\n",
        "        print(\"Snippets:\", snippets)\n",
        "\n",
        "    return names, urls, snippets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz-k4GuMiUbg"
      },
      "source": [
        "#### AI MODELS API INTEGRATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIxrwZTJiUbg"
      },
      "outputs": [],
      "source": [
        "class AI():\n",
        "\n",
        "    def DeepSeek(system_prompt, query):\n",
        "        client = OpenAI(\n",
        "            api_key=DEEPSEEK_API,\n",
        "            base_url=\"https://api.deepseek.com/v1\")\n",
        "\n",
        "        llm_response = client.chat.completions.create(\n",
        "                model=\"deepseek-chat\",\n",
        "                messages=[\n",
        "                        {\"role\": \"system\", \"content\": system_prompt},\n",
        "                        {\"role\": \"user\", \"content\": query},\n",
        "                    ],\n",
        "                    max_tokens=1024,\n",
        "                    stop=stop_words,\n",
        "                    stream=True,\n",
        "                    temperature=0.9,\n",
        "                )\n",
        "\n",
        "        # Initialize an empty list to accumulate chunks\n",
        "        chunks = []\n",
        "\n",
        "        # Print real-time response and accumulate chunks\n",
        "        for chunk in llm_response:\n",
        "            if chunk.choices[0].delta.content is not None:\n",
        "                # Print real-time response\n",
        "                print(chunk.choices[0].delta.content, end=\"\")\n",
        "\n",
        "                # Accumulate chunk\n",
        "                chunks.append(chunk.choices[0].delta.content)\n",
        "\n",
        "\n",
        "        print(\"\\n\\n\")\n",
        "        # Join chunks together to form the complete response\n",
        "        complete_response = ''.join(chunks)\n",
        "\n",
        "        return complete_response\n",
        "\n",
        "\n",
        "    def Lepton(system_prompt, query):\n",
        "        client = OpenAI(\n",
        "        base_url=\"https://mixtral-8x7b.lepton.run/api/v1/\",\n",
        "        api_key=LEPTON_API\n",
        "        )\n",
        "\n",
        "        llm_response = client.chat.completions.create(\n",
        "            model=\"mixtral-8x7b\",\n",
        "            messages=[\n",
        "                        {\"role\": \"system\", \"content\": system_prompt},\n",
        "                        {\"role\": \"user\", \"content\": query},\n",
        "                    ],\n",
        "                    max_tokens=1024,\n",
        "                    stop=stop_words,\n",
        "                    stream=True,\n",
        "                    temperature=0.9,\n",
        "                )\n",
        "\n",
        "        # Initialize an empty list to accumulate chunks\n",
        "        chunks = []\n",
        "\n",
        "        # Print real-time response and accumulate chunks\n",
        "        for chunk in llm_response:\n",
        "\n",
        "            try:\n",
        "                if chunk.choices[0].delta.content is not None:\n",
        "                    # Print real-time response\n",
        "                    print(chunk.choices[0].delta.content, end=\"\")\n",
        "                    # Accumulate chunk\n",
        "                    chunks.append(chunk.choices[0].delta.content)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "\n",
        "        print(\"\\n\\n\")\n",
        "        # Join chunks together to form the complete response\n",
        "        complete_response = ''.join(chunks)\n",
        "\n",
        "        return complete_response\n",
        "\n",
        "\n",
        "    def Groq(system_prompt, query):\n",
        "\n",
        "        client = OpenAI(\n",
        "            base_url = \"https://api.groq.com/openai/v1\",\n",
        "            api_key=GROQ_API\n",
        "            )\n",
        "        llm_response = client.chat.completions.create(\n",
        "            model=\"llama2-70b-4096\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": system_prompt\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": query\n",
        "                }\n",
        "            ],\n",
        "            temperature=0.5,\n",
        "            max_tokens=1024,\n",
        "            top_p=1,\n",
        "            stream=True,\n",
        "            stop=None,\n",
        "        )\n",
        "\n",
        "        # Initialize an empty list to accumulate chunks\n",
        "        chunks = []\n",
        "\n",
        "        # Print real-time response and accumulate chunks\n",
        "        for chunk in llm_response:\n",
        "\n",
        "            try:\n",
        "                if chunk.choices[0].delta.content is not None:\n",
        "                    # Print real-time response\n",
        "                    print(chunk.choices[0].delta.content, end=\"\")\n",
        "                    # Accumulate chunk\n",
        "                    chunks.append(chunk.choices[0].delta.content)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "\n",
        "        print(\"\\n\\n\")\n",
        "        # Join chunks together to form the complete response\n",
        "        complete_response = ''.join(chunks)\n",
        "\n",
        "        return complete_response\n",
        "\n",
        "\n",
        "    def Openai(system_prompt, query):\n",
        "\n",
        "        client = OpenAI(\n",
        "            base_url = \"https://api.openai.com/v1\",\n",
        "            api_key=OpenAI_API\n",
        "            )\n",
        "        llm_response = client.chat.completions.create(\n",
        "            model=\"llama2-70b-4096\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": system_prompt\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": query\n",
        "                }\n",
        "            ],\n",
        "            temperature=0.5,\n",
        "            max_tokens=1024,\n",
        "            top_p=1,\n",
        "            stream=True,\n",
        "            stop=None,\n",
        "        )\n",
        "\n",
        "        # Initialize an empty list to accumulate chunks\n",
        "        chunks = []\n",
        "\n",
        "        # Print real-time response and accumulate chunks\n",
        "        for chunk in llm_response:\n",
        "\n",
        "            try:\n",
        "                if chunk.choices[0].delta.content is not None:\n",
        "                    # Print real-time response\n",
        "                    print(chunk.choices[0].delta.content, end=\"\")\n",
        "                    # Accumulate chunk\n",
        "                    chunks.append(chunk.choices[0].delta.content)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "\n",
        "        print(\"\\n\\n\")\n",
        "        # Join chunks together to form the complete response\n",
        "        complete_response = ''.join(chunks)\n",
        "\n",
        "        return complete_response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRdnbb-yiUbi"
      },
      "source": [
        "#### FUNCTION CALLS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITXgxFkNiUbi"
      },
      "outputs": [],
      "source": [
        "def get_related_questions(query, contexts):\n",
        "\n",
        "        system_prompt = _more_questions_prompt.format(\n",
        "                            context=\"\\n\\n\".join([c[\"snippet\"] for c in contexts])\n",
        "                        )\n",
        "\n",
        "        try:\n",
        "            # complete_response = AI.Lepton(system_prompt, query.)\n",
        "            # complete_response = AI.DeepSeek(system_prompt, query)\n",
        "            complete_response = AI.Groq(system_prompt, query)\n",
        "            return complete_response\n",
        "\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            # For any exceptions, we will just return an empty list.\n",
        "            return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPtJfoQviUbi"
      },
      "outputs": [],
      "source": [
        "def generate_answer(query, contexts):\n",
        "\n",
        "    # Basic attack protection: remove \"[INST]\" or \"[/INST]\" from the query\n",
        "    query = re.sub(r\"\\[/?INST\\]\", \"\", query)\n",
        "\n",
        "    system_prompt = _rag_query_text.format(\n",
        "                context=\"\\n\\n\".join(\n",
        "                    [f\"[[citation:{i+1}]] {c['snippet']}\" for i, c in enumerate(contexts)]\n",
        "                )\n",
        "            )\n",
        "\n",
        "    try:\n",
        "        # complete_response = AI.Lepton(system_prompt, query)\n",
        "        # complete_response = AI.DeepSeek(system_prompt, query)\n",
        "        complete_response = AI.Groq(system_prompt, query)\n",
        "        return complete_response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return \"Failed Response\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwuVb9m0iUbi"
      },
      "outputs": [],
      "source": [
        "def main(query, contexts, urls):\n",
        "\n",
        "    print(\"Sources ---->\")\n",
        "    for _url in urls:\n",
        "        print(_url)\n",
        "\n",
        "    print(\"\\n\\nAnswers --->\")\n",
        "    citations = extract_citation_numbers(generate_answer(query, contexts))\n",
        "    print('\\n'.join([f\"Citation : {citation} --->  {urls[int(citation)-1]}\" for citation in citations]))\n",
        "\n",
        "\n",
        "    print(\"\\n\\nRelated Questions --->\")\n",
        "    get_related_questions(query, contexts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJsjArz7iUbj"
      },
      "source": [
        "#### RESULTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YD2UUhddiUbj"
      },
      "outputs": [],
      "source": [
        "query = \"who won the match between RCB and CSK in the latest Ipl. Provide some more information of the Match\"\n",
        "contexts = search_with_serper(query)\n",
        "name, url, snippets = fetch_json_attributes(contexts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOv3XXBXiUbj",
        "outputId": "ddd0f277-c3a4-46b4-c1e8-a39e1d1c5c72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sources ---->\n",
            "https://indianexpress.com/article/sports/cricket/csk-vs-rcb-live-score-ipl-2024-match-01-chennai-super-kings-vs-royal-challengers-bengaluru-scorecard-updates-9228174/\n",
            "https://sportstar.thehindu.com/cricket/ipl/ipl-news/csk-vs-rcb-live-score-ipl-2024-chennai-super-kings-royal-challengers-bangalore-match-updates-streaming-info-highlights/article67979676.ece\n",
            "https://www.news18.com/cricket/csk-vs-rcb-live-score-ipl-2024-match-1-chennai-super-kings-vs-royal-challengers-bangalore-scorcard-updates-2-liveblog-8825017.html\n",
            "https://www.espncricinfo.com/series/indian-premier-league-2024-1410320/chennai-super-kings-vs-royal-challengers-bengaluru-1st-match-1422119/match-report\n",
            "https://zeenews.india.com/cricket/live-updates/highlights-ipl-2024-opening-ceremony-csk-vs-rcb-1st-match-cricket-score-and-updates-ms-dhoni-ruturaj-gaikwad-vs-virat-kohli-ma-chidambaram-stadium-indian-premier-league-live-streaming-details-csk-beat-rcb-by-6-wickets-2732812\n",
            "https://www.moneycontrol.com/sports/cricket/ipl/csk-vs-rcb-ipl-2024-live-score-chennai-super-kings-vs-royal-challengers-bangalore-ipl-toss-updates-latest-match-scorecard-liveblog-12509791.html\n",
            "https://sports.ndtv.com/ipl-2024/csk-vs-rcb-ipl-live-score-ipl-2024-22-march-chennai-super-kings-vs-royal-challengers-bengaluru-live-updates-5290015\n",
            "https://www.espncricinfo.com/series/indian-premier-league-2024-1410320/chennai-super-kings-vs-royal-challengers-bengaluru-1st-match-1422119/live-cricket-score\n",
            "\n",
            "\n",
            "\n",
            "The latest match between RCB and CSK was won by CSK. According to [[citation:1]], Chennai Super Kings started their 2024 campaign with a victory. In addition, [[citation:2]] states that CSK won by six wickets, and [[citation:5]] confirms that CSK won the contest by six wickets against the Royal Challengers Bengaluru.\n",
            "\n",
            "\n",
            "In the match, Mustafizur Rahman took a four-fer, and Shivam Dube scored 34* to help CSK chase down 174, as reported in [[citation:3]]. Rachin Ravindra and Shivam Dube played key roles with the bat, as stated in [[citation:4]].\n",
            "\n",
            "On the other hand, RCB has yet to win their first IPL title or IPL 2024, despite being favorites to win for many years, as mentioned in [[citation:6]].\n",
            "\n",
            "For more information, you can check the scoreboard, ball-by-ball commentary, and updates on ESPNcricinfo.com, as suggested in [[citation:8]].\n",
            "\n",
            "\n",
            "Citation : 1 --->  https://indianexpress.com/article/sports/cricket/csk-vs-rcb-live-score-ipl-2024-match-01-chennai-super-kings-vs-royal-challengers-bengaluru-scorecard-updates-9228174/\n",
            "Citation : 2 --->  https://sportstar.thehindu.com/cricket/ipl/ipl-news/csk-vs-rcb-live-score-ipl-2024-chennai-super-kings-royal-challengers-bangalore-match-updates-streaming-info-highlights/article67979676.ece\n",
            "Citation : 5 --->  https://zeenews.india.com/cricket/live-updates/highlights-ipl-2024-opening-ceremony-csk-vs-rcb-1st-match-cricket-score-and-updates-ms-dhoni-ruturaj-gaikwad-vs-virat-kohli-ma-chidambaram-stadium-indian-premier-league-live-streaming-details-csk-beat-rcb-by-6-wickets-2732812\n",
            "Citation : 3 --->  https://www.news18.com/cricket/csk-vs-rcb-live-score-ipl-2024-match-1-chennai-super-kings-vs-royal-challengers-bangalore-scorcard-updates-2-liveblog-8825017.html\n",
            "Citation : 4 --->  https://www.espncricinfo.com/series/indian-premier-league-2024-1410320/chennai-super-kings-vs-royal-challengers-bengaluru-1st-match-1422119/match-report\n",
            "Citation : 6 --->  https://www.moneycontrol.com/sports/cricket/ipl/csk-vs-rcb-ipl-2024-live-score-chennai-super-kings-vs-royal-challengers-bangalore-ipl-toss-updates-latest-match-scorecard-liveblog-12509791.html\n",
            "Citation : 8 --->  https://www.espncricinfo.com/series/indian-premier-league-2024-1410320/chennai-super-kings-vs-royal-challengers-bengaluru-1st-match-1422119/live-cricket-score\n",
            "\n",
            "\n",
            "Related Questions\n",
            "Sure, here are three follow-up questions based on the given context:\n",
            "\n",
            "1. What was the score of RCB in the match against CSK?\n",
            "2. How many wickets did Mustafizur Rahman take in the match?\n",
            "3. Who was the top scorer for CSK in the match?\n",
            "\n",
            "Here are the questions in the format requested:\n",
            "\n",
            "1. What was RCB's score in CSK vs RCB?\n",
            "2. How many wickets did Mustafizur Rahman take in CSK vs RCB?\n",
            "3. Who was CSK's top scorer in CSK vs RCB?\n",
            "\n",
            "I hope this helps! Let me know if you need further assistance.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "main(query, contexts, url)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}